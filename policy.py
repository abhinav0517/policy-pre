# -*- coding: utf-8 -*-
"""policy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yTZrkxFWqq08SSqJS-GfoP9IIOCXcWR1
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, roc_curve
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# Load and inspect data
df = pd.read_csv('insurance3r2.csv')  # Ensure the correct file path
print("Data shape:", df.shape)
print("\nFirst 5 rows:\n", df.head())

# Basic data checks
print("\nMissing values:\n", df.isnull().sum())
print("\nData types:\n", df.dtypes)

# Encode categorical variables
le = LabelEncoder()
df['sex'] = le.fit_transform(df['sex'])
df['smoker'] = le.fit_transform(df['smoker'])
df = pd.get_dummies(df, columns=['region'], drop_first=True)

# Exploratory Data Analysis (EDA)
plt.figure(figsize=(12, 5))
sns.countplot(x='insuranceclaim', data=df, palette='coolwarm')
plt.title('Distribution of Insurance Claims')
plt.show()

plt.figure(figsize=(12, 5))
sns.boxplot(x='insuranceclaim', y='charges', data=df, palette='coolwarm')
plt.title('Insurance Claims vs Charges')
plt.show()

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Feature Engineering: BMI Category
df['bmi_category'] = pd.cut(df['bmi'],
                            bins=[0, 18.5, 25, 30, 100],
                            labels=['Underweight', 'Normal', 'Overweight', 'Obese'])
df = pd.get_dummies(df, columns=['bmi_category'], drop_first=True)

# Prepare data for modeling
X = df.drop('insuranceclaim', axis=1)
y = df['insuranceclaim']

# Split data into training and testing sets (Stratified for class balance)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42, stratify=y)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to train and evaluate models
def train_evaluate_model(model, param_grid=None):
    if param_grid:
        grid = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')
        grid.fit(X_train_scaled, y_train)
        best_model = grid.best_estimator_
        print(f"Best parameters: {grid.best_params_}")
    else:
        best_model = model
        best_model.fit(X_train_scaled, y_train)

    # Predictions
    y_pred = best_model.predict(X_test_scaled)
    y_proba = best_model.predict_proba(X_test_scaled)[:, 1]

    # Evaluation metrics
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    print(f"ROC AUC Score: {roc_auc_score(y_test, y_proba):.3f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.show()

    return best_model

# Train & Evaluate Random Forest
print("\nRandom Forest Model:")
rf_params = {'n_estimators': [100, 200], 'max_depth': [None, 5, 10]}
rf_model = train_evaluate_model(RandomForestClassifier(), rf_params)

# Train & Evaluate XGBoost (Fixed Warning)
print("\nXGBoost Model:")
xgb_params = {'learning_rate': [0.01, 0.1], 'max_depth': [3, 5], 'n_estimators': [100, 200]}
xgb_model = train_evaluate_model(XGBClassifier(eval_metric='logloss'), xgb_params)

# Train & Evaluate Logistic Regression
print("\nLogistic Regression Model:")
logreg_model = train_evaluate_model(LogisticRegression(solver='liblinear'))

# Model Comparison: ROC Curve
final_models = {'Random Forest': rf_model, 'XGBoost': xgb_model, 'Logistic Regression': logreg_model}

plt.figure(figsize=(10, 6))
for name, model in final_models.items():
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test_scaled)[:, 1])
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:,1]):.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.show()

# Feature Importance (For Random Forest)
feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)
feature_importance.nlargest(10).plot(kind='barh', color='steelblue')
plt.title('Top 10 Important Features')
plt.show()

# Project Summary
print("\nProject Summary:")
print("- Best performing model: Likely XGBoost or Random Forest (Check AUC score)")
print("- Key predictors: Charges, BMI, Age, Smoker Status")
print("- Next steps: Fine-tune hyperparameters and explore cost-sensitive learning for fraud detection")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, roc_curve
from xgboost import XGBClassifier

# Load and inspect data
df = pd.read_csv('/content/insurance3r2.csv')
print("Data shape:", df.shape)

# Encode categorical variables
le = LabelEncoder()
df['sex'] = le.fit_transform(df['sex'])
df['smoker'] = le.fit_transform(df['smoker'])
df = pd.get_dummies(df, columns=['region'], drop_first=True)

# Feature Engineering
df['bmi_category'] = pd.cut(df['bmi'], bins=[0, 18.5, 25, 30, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])
df = pd.get_dummies(df, columns=['bmi_category'], drop_first=True)

# Prepare Data
X = df.drop('insuranceclaim', axis=1)
y = df['insuranceclaim']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle Class Imbalance
pos_weight = (y == 0).sum() / (y == 1).sum()

# Define XGBoost Model (Without Deprecated Parameter)
xgb = XGBClassifier(objective='binary:logistic', eval_metric='logloss', scale_pos_weight=pos_weight)

# Hyperparameter tuning
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)
grid_search.fit(X_train_scaled, y_train)

# Best Model
best_xgb = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Predictions
y_pred = best_xgb.predict(X_test_scaled)
y_proba = best_xgb.predict_proba(X_test_scaled)[:,1]

# Model Evaluation
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print(f"ROC AUC Score: {roc_auc_score(y_test, y_proba):.3f}")

# Confusion Matrix
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc_score(y_test, y_proba):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Feature Importance
plt.figure(figsize=(10,6))
feature_importance = pd.Series(best_xgb.feature_importances_, index=X.columns)
feature_importance.nlargest(10).plot(kind='barh', color='blue')
plt.title('Top 10 Important Features')
plt.show()

